{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Activation Functions in Neural Networks**\n",
        "\n",
        "https://www.mygreatlearning.com/blog/activation-functions/\n",
        "\n",
        "Activation functions play a crucial role in neural networks by determining whether a neuron should be activated or not. They introduce non-linearity, allowing networks to learn complex patterns. Without activation functions, a neural network would behave like a simple linear model, limiting its ability to solve real-world problems.\n"
      ],
      "metadata": {
        "id": "pxHzHi_5-5BT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What Are Activation Functions?**\n",
        "\n",
        "An activation function is a mathematical function applied to a neuron’s input to decide its output. It transforms the weighted sum of inputs into an output signal that is passed to the next layer in a neural network. The function’s primary objective is to introduce non-linearity into the network, enabling it to learn complex representations.\n",
        "\n",
        "Without activation functions, a neural network could only model linear relationships, making it ineffective for solving non-trivial problems such as image classification, speech recognition, and natural language processing."
      ],
      "metadata": {
        "id": "yACnYY1RV1Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Why Are Activation Functions Necessary?**\n",
        "\n",
        "Neural networks consist of multiple layers where neurons process input signals and pass them to subsequent layers. Everything inside a neural network becomes a basic linear transformation when activation functions are removed, which renders the network unable to discover complex features.\n",
        "\n",
        "Key reasons why activation functions are necessary:\n",
        "\n",
        "1. Introduce non-linearity: Real-world problems often involve complex, non-linear relationships. Activation functions enable neural networks to model these relationships.\n",
        "2. Enable hierarchical feature learning: Deep networks extract multiple levels of features from raw data, making them more powerful for pattern recognition.\n",
        "\n",
        "3. Prevent network collapse: Without activation functions, every layer would perform just a weighted sum, reducing the depth of the network into a single linear model.\n",
        "4. Improve convergence during training: Certain activation functions help improve gradient flow, ensuring faster and more stable learning."
      ],
      "metadata": {
        "id": "wTDCOCO_V-qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Types of Activation Functions**\n",
        "1. Linear Activation Function\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-1.png\">\n",
        "\n",
        "*Formula: f(x) = ax*\n",
        "\n",
        "* The functioning produces an input value that has undergone scaling.\n",
        "* The lack of non-linear elements prevents the network from fulfilling its complete learning capacity.\n",
        "* Deep learning practitioners only infrequently use this activation function because it functions as a linear regression model.\n",
        "* Use case: Often used in regression-based models where predicting continuous values is necessary."
      ],
      "metadata": {
        "id": "jr03F3U2oliQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sigmoid Activation Function\n",
        "\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-4.png\">\n",
        "\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-3.png\">\n",
        "\n",
        "* Outputs values between 0 and 1.\n",
        "* Useful for probability-based models like binary classification.\n",
        "* Advantages: Smooth gradient, well-defined range, and interpretable output as probabilities.\n",
        "* Drawbacks: Prone to the vanishing gradient problem, leading to slow learning in deep networks. It is also computationally expensive due to the exponentiation operation."
      ],
      "metadata": {
        "id": "YLp8ZXeppYT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tanh (Hyperbolic Tangent) Activation Function\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-6.png\">\n",
        "\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-5.png\">\n",
        "\n",
        "* Outputs values between -1 and 1.\n",
        "* Centers the data around zero, helping in better gradient flow.\n",
        "* Advantages: Scaled tanh activation offers enhanced gradient propagation since it operates from the zero-centered range.\n",
        "* Drawbacks: The training of deep models becomes difficult because the deep networks experience reduced gradient propagation despite overcoming the sigmoid vanishing gradient problem."
      ],
      "metadata": {
        "id": "It1yQ2VisnWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ReLU (Rectified Linear Unit) Activation Function\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-8.png\">\n",
        "\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-7.png\">\n",
        "\n",
        "* The most commonly used activation function in deep learning.\n",
        "* Introduces non-linearity while avoiding the vanishing gradient problem.\n",
        "* Advantages: Computationally efficient and prevents gradient saturation.\n",
        "* Drawbacks: The implementation of “dying ReLU” results in dying neurons that stop learning because they become inactive when receiving negative inputs."
      ],
      "metadata": {
        "id": "sxvORHdLtvNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Leaky ReLU Activation Function\n",
        "\n",
        "<img src=\"https://dotnettutorials.net/wp-content/uploads/2022/06/word-image-27976-9.png\">\n",
        "\n",
        "<img src=\"https://www.mygreatlearning.com/blog/wp-content/uploads/2020/08/leaky-relu-formula.png.webp\">\n",
        "\n",
        "* A modified version of ReLU to allow small gradients for negative inputs.\n",
        "* Helps to prevent dying neurons.\n",
        "* Advantages: Maintains non-linearity while addressing ReLU’s limitation.\n",
        "* Drawbacks: Choosing the best negative slope value is not always straightforward. Performance varies across different datasets."
      ],
      "metadata": {
        "id": "09F7Wi0quYC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ELU (Exponential Linear Unit) Activation Function\n",
        "<img src=\"https://raw.githubusercontent.com/mmuratarat/mmuratarat.github.io/master/_posts/images/elu_plot.png\">\n",
        "<img src=\"https://www.mygreatlearning.com/blog/wp-content/uploads/2020/08/elu-activation-formula.png.webp\">\n",
        "\n",
        "* The dying ReLU problem receives a solution because the activation function accepts small negative values.\n",
        "* Advantages: Provides smooth gradient propagation and speeds up learning.\n",
        "* Drawbacks: Computationally more expensive than ReLU, which can be an issue in large-scale applications.\n"
      ],
      "metadata": {
        "id": "So0Es9_KvO_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Softmax Activation Function\n",
        "\n",
        "<img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=1024&disable=upscale&auto=webp\">\n",
        "\n",
        "<img src=\"https://velog.velcdn.com/images/chiroya/post/b520fcca-ce29-4b02-9392-5de67767e6b4/image.png\">\n",
        "\n",
        "<img src=\"https://www.mygreatlearning.com/blog/wp-content/uploads/2020/08/softmax-activation-function-formula.png.webp\">\n",
        "\n",
        "* Used in multi-class classification problems.\n",
        "* Converts logits into probabilities.\n",
        "* Advantages: Ensures sum of probabilities equals 1, making it interpretable for classification tasks.\n",
        "* Drawbacks: Computationally expensive and sensitive to outliers, as large input values can dominate the output.\n",
        "\n",
        "\n",
        "<img src=\"https://www.mygreatlearning.com/blog/wp-content/uploads/2020/08/choosing-activation-function-1.png.webp\">"
      ],
      "metadata": {
        "id": "QPTysh4RyO1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "obJTtWXn-uM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJpKnSWN6jfz"
      },
      "outputs": [],
      "source": [
        "# sigmoid function\n",
        "def sigmoid(z):\n",
        "  return 1.0 / (1 + np.exp(-z))\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_prime(z):\n",
        "  return sigmoid(z) * (1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh activation function\n",
        "def tanh(z):\n",
        "\treturn (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
        "# Derivative of Tanh Activation Function\n",
        "def tanh_prime(z):\n",
        "\treturn 1 - np.power(tanh(z), 2)"
      ],
      "metadata": {
        "id": "ZdhsG5m1-buo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation function\n",
        "def relu(z):\n",
        "  return max(0, z)\n",
        "# Derivative of ReLU Activation Function\n",
        "def relu_prime(z):\n",
        "  return 1 if z > 0 else 0\n"
      ],
      "metadata": {
        "id": "usSylr7z-luq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leaky_ReLU activation function\n",
        "def leakyrelu(z, alpha):\n",
        "\treturn max(alpha * z, z)\n",
        "# Derivative of leaky_ReLU Activation Function\n",
        "def leakyrelu_prime(z, alpha):\n",
        "\treturn 1 if z > 0 else alpha"
      ],
      "metadata": {
        "id": "xRG3qG3N-pHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "scores = [3.0, 1.0, 0.2]\n",
        "print(softmax(scores))"
      ],
      "metadata": {
        "id": "-2f5sLPC-s2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290965e0-4a12-430e-fcba-2f1b35ab163d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8360188  0.11314284 0.05083836]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Activativation functions using Tensorflow**"
      ],
      "metadata": {
        "id": "g5DJUUjDI9PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# ReLU activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3znSv96iI75J",
        "outputId": "97d823f1-4cc7-4aea-b190-a8b6b3e1d782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Sigmoid activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Used for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "A8qHCnCtJI2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Tanh activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='tanh', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hACp-tGzJRvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# ELU activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='elu', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TnE3HyJDJeCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Leaky ReLU activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, input_shape=(32,)),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.3),  # Allows a small slope for negative values\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IivedMw8JZyu",
        "outputId": "e3102f09-8fda-4b1b-d476-ebaea5bf82e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Softmax activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # For multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "alVtcEGsJUA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Swish activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='swish', input_shape=(32,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "C1Uqt9iKJWZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8dcv3PRJivZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}